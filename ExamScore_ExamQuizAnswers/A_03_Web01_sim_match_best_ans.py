import os
import re
import pandas as pd
import numpy as np
from functools import reduce
import shutil
from pathlib import Path


def clean_text(text):
    return re.sub(r'[\s\u3000]', '', text)


def longest_common_substring(s1, s2):
    m, n = len(s1), len(s2)
    dp = [[0] * (n + 1) for _ in range(m + 1)]
    max_len = 0
    for i in range(m):
        for j in range(n):
            if s1[i] == s2[j]:
                dp[i + 1][j + 1] = dp[i][j] + 1
                max_len = max(max_len, dp[i + 1][j + 1])
    return max_len


def split_into_segments(text):
    raw_segments = re.split(r'[ã€‚ï¼ï¼Ÿ\n]+', text)
    segments = [seg.strip() for seg in raw_segments if len(seg.strip()) >= 10]
    return segments


def compare_segments_recursive(txt_root_folder, html_path, match_suffix, threshold=0.5):
    with open(html_path, 'r', encoding='utf-8') as f:
        html_content = clean_text(f.read())

    result_summary = {}
    result_table = []

    for root, dirs, files in os.walk(txt_root_folder):
        for file in files:
            if file.endswith(match_suffix):
                full_path = os.path.join(root, file)
                with open(full_path, 'r', encoding='utf-8') as f:
                    txt_content = f.read()

                segments = split_into_segments(txt_content)
                similarities = []

                # print(f"\næ–‡ä»¶ï¼š{file}ï¼ˆå…± {len(segments)} æ®µï¼‰ | è·¯å¾„ï¼š{full_path}")

                for idx, segment in enumerate(segments, 1):
                    cleaned_seg = clean_text(segment)
                    lcs_len = longest_common_substring(cleaned_seg, html_content)
                    similarity = lcs_len / max(len(cleaned_seg), 1)
                    similarities.append(similarity)
                    preview = segment[:30].replace('\n', '')
                    # print(f"  æ®µ {idx:02d} ç›¸ä¼¼åº¦ï¼š{similarity:.2%} | å†…å®¹å‰30å­—ï¼š{preview}")

                if similarities:
                    max_sim = max(similarities)
                    avg_sim = sum(similarities) / len(similarities)
                    count_over_threshold = sum(1 for s in similarities if s >= threshold)

                    result_summary[full_path] = {
                        'max_similarity': round(max_sim, 4),
                        'avg_similarity': round(avg_sim, 4),
                        'match_count': count_over_threshold,
                        'total_segments': len(similarities)
                    }

                    result_table.append({
                        'æ–‡ä»¶å': file,
                        'å¹³å‡ç›¸ä¼¼åº¦': round(avg_sim, 4)
                    })

    df = pd.DataFrame(result_table)
    return result_summary, df


def build_per_question_similarity_matrix(txt_folder_path, html_path, skip_question, num_question):
    result_list = []
    for i in range(skip_question + 1, skip_question + num_question + 1):
        suffix = f'_0{i:02}.txt'
        _, similarity_df = compare_segments_recursive(txt_folder_path, html_path, match_suffix=suffix)

        if similarity_df is None or similarity_df.empty:
            print(f"è·³è¿‡ç©ºæ–‡ä»¶ï¼š{suffix}")
            continue

        df = similarity_df.copy().reset_index(drop=True)

        # è‡ªåŠ¨æ‰¾å‡ºæ–‡ä»¶ååˆ—
        filename_col = None
        for col in df.columns:
            if df[col].astype(str).str.contains(r'\.txt$').any():
                filename_col = col
                break
        if filename_col is None:
            raise ValueError(f"{suffix} æ²¡æœ‰å‘ç°æ–‡ä»¶ååˆ—")

        # å¾—åˆ†åˆ—ï¼ˆé€‰é¡¹ï¼‰
        score_cols = [col for col in df.columns if col != filename_col]

        # è·å– basename å¹¶å»ºç«‹æ–° df
        basename = df[filename_col].apply(lambda x: re.sub(r'_0\d{2}\.txt$', '', str(x)))
        score_df = df[score_cols].copy()
        score_df.columns = [f"{i:02}_{col}" for col in score_cols]  # åŠ é¢˜å·å‰ç¼€

        score_df.insert(0, 'basename', basename)  # åŠ ä¸Š basename

        result_list.append(score_df)

    if not result_list:
        print("âš ï¸ æ‰€æœ‰æ–‡ä»¶ä¸ºç©º")
        return pd.DataFrame()

    # merge æ‰€æœ‰é¢˜çš„å¾—åˆ†åˆ—ï¼ˆæŒ‰ basename å¯¹é½ï¼‰
    final_df = reduce(lambda left, right: pd.merge(left, right, on='basename', how='outer'), result_list)
    final_df.sort_values(by='basename', inplace=True)
    return final_df


def add_max_column_name(df):
    score_only = df.iloc[:, 1:]  # é™¤ basename å¤–çš„æ‰€æœ‰é¢˜ç›®+é€‰é¡¹åˆ—
    max_col = score_only.idxmax(axis=1)  # æ‰¾å‡ºæ¯è¡Œæœ€å¤§å€¼æ‰€åœ¨åˆ—å
    df['max_col'] = max_col
    return df


def build_similarity_tensor(exam_file_temp, html_path, html_file_base_name, skip_question, num_question):
    df_list = []

    for i_quiz in range(len(html_file_base_name)):
        html_path_temp = os.path.join(html_path, html_file_base_name[i_quiz] + ".html")

        # æ„é€ çŸ©é˜µï¼ˆç›¸ä¼¼åº¦å€¼ï¼‰
        matrix = build_per_question_similarity_matrix(exam_file_temp, html_path_temp, skip_question, num_question)

        # å»é™¤ max_colï¼Œåªä¿ç•™ basename å’Œå„é¢˜åˆ—
        matrix_clean = matrix.drop(columns=['max_col'], errors='ignore')
        df_list.append(matrix_clean)

    # è·å–ç»Ÿä¸€çš„ basename å’Œé¢˜å·åˆ—è¡¨
    all_basenames = sorted(set().union(*[set(df['basename']) for df in df_list]))
    all_questions = df_list[0].columns[1:]  # ç¬¬ä¸€åˆ—æ˜¯ basenameï¼Œåé¢æ˜¯ 10,11,...

    # åˆå§‹åŒ–å¼ é‡ (å­¦ç”Ÿæ•° Ã— é¢˜æ•° Ã— htmlç‰ˆæœ¬æ•°)
    tensor = np.full((len(all_basenames), len(all_questions), len(df_list)), np.nan)

    basename_to_idx = {b: i for i, b in enumerate(all_basenames)}
    question_to_idx = {q: i for i, q in enumerate(all_questions)}

    for k, df in enumerate(df_list):
        for _, row in df.iterrows():
            bname = row['basename']
            for q in all_questions:
                tensor[basename_to_idx[bname], question_to_idx[q], k] = row[q]

    return tensor, all_basenames, all_questions


def find_max_similarity_question_indices(tensor):
    """
    å¯¹äºæ¯ä½å­¦ç”Ÿ i å’Œæ¯ä¸ª htmlç‰ˆæœ¬ vï¼Œè¿”å›ç›¸ä¼¼åº¦æœ€é«˜çš„é¢˜ç›®ç´¢å¼• qã€‚
    è¾“å‡º shape ä¸º (N, V)
    """
    N, Q, V = tensor.shape
    result = []

    for i in range(N):
        max_indices = []
        for v in range(V):
            col = tensor[i, :, v]
            if np.isnan(col).all():
                max_indices.append(-1)
            else:
                max_indices.append(int(np.nanargmax(col)))
        result.append(max_indices)

    return pd.DataFrame(result)


def sim_match_best_ans(txt_folder_path, exam_file_base_name, html_path, html_file_base_name, skip_question, num_question):
    for i_class in range(len(exam_file_base_name)):
        exam_file_temp = os.path.join(txt_folder_path, exam_file_base_name[i_class])
        print(exam_file_temp)

        try:
            tensor, students, questions = build_similarity_tensor(
                exam_file_temp, html_path, html_file_base_name, skip_question, num_question)
        except Exception as e:
            print(f"âŒ æ„å»ºç›¸ä¼¼åº¦å¼ é‡å¤±è´¥ï¼š{exam_file_temp}")
            print(html_path)
            print(html_file_base_name)
            print(skip_question)
            print(num_question)
            print(f"é”™è¯¯è¯¦æƒ…ï¼š{e}")
            continue  # âœ… è·³è¿‡å½“å‰å¾ªç¯ï¼Œä¸å†æ‰§è¡Œåç»­é€»è¾‘

        question_max_df = find_max_similarity_question_indices(tensor) + skip_question + 1
        question_max_df.columns = [f"page{i + 1}" for i in range(question_max_df.shape[1])]
        question_max_df.insert(0, "basename", students)

        # ä¿å­˜ä¸º Excel æ–‡ä»¶
        excel_file_name = os.path.join(txt_folder_path, exam_file_base_name[i_class] + "_ç›¸ä¼¼æ€§.xlsx")
        question_max_df.to_excel(excel_file_name, index=False)



# é˜²æ­¢ä¸­è‹±æ–‡æ‹¬å·ä¸åŒ¹é…
def normalize_parentheses_pattern(text: str) -> str:
    escaped = re.escape(text)
    return re.sub(r"\\[ï¼ˆ(].+?\\[)ï¼‰]", r"[(ï¼ˆ][^()ï¼ˆï¼‰]+[)ï¼‰]", escaped)



# move best match file according to excel dictionary
def copy_and_rename_by_similarity_excel(
    input_folder: str,
    output_folder: str,
    excel_path: str
):
    """
    é€’å½’æ‰«æ input_folder ä¸­æ‰€æœ‰ _0XX.txt æ–‡ä»¶ï¼Œæ ¹æ® excel_path ä¸­çš„ç›¸ä¼¼æ€§è¡¨æ ¼ï¼Œ
    æ‰¾å‡ºå¯¹åº”çš„ pageXï¼Œå¤åˆ¶æ–‡ä»¶å¹¶å°†å…¶å‘½åä¸º _pageX.txt åˆ° output_folderã€‚
    """

    os.makedirs(output_folder, exist_ok=True)
    df = pd.read_excel(excel_path)

    # è‡ªåŠ¨è¯†åˆ« pageX åˆ—
    page_cols = [col for col in df.columns if str(col).startswith("page")]

    # é€’å½’æŸ¥æ‰¾æ‰€æœ‰ .txt æ–‡ä»¶
    for root, _, files in os.walk(input_folder):
        for filename in files:
            if not filename.endswith(".txt") or "_0" not in filename:
                continue

            full_path = os.path.join(root, filename)
            basename_match = filename.rsplit("_0", 1)[0]
            suffix = filename[-7:-4]  # å– '013'

            # æŸ¥æ‰¾ DataFrame åŒ¹é…çš„ basename è¡Œ  row = df[df["basename"].str.contains(basename_match)]
            pattern = normalize_parentheses_pattern(basename_match)
            row = df[df["basename"].str.contains(pattern, regex=True, na=False)]

            if row.empty:
                print(f"âŒ æœªæ‰¾åˆ°åŒ¹é… basename: {basename_match}")
                continue

            row = row.iloc[0]
            new_page = None

            for col in page_cols:
                if str(row[col]).zfill(3) == suffix:
                    new_page = col
                    break

            if new_page is None:
                print(f"â— æœªåŒ¹é…é¢˜å· {suffix} äº {basename_match}")
                continue

            new_filename = f"{basename_match}_{new_page}.txt"
            new_path = os.path.join(output_folder, new_filename)

            shutil.copy(full_path, new_path)
            # print(f"âœ… å·²å¤åˆ¶å¹¶é‡å‘½åï¼š{filename} â†’ {new_filename}")



# move best match file according to excel dictionary
def batch_similarity_move_by_excel(exam_folder_sim_excel_path: str, exam_folder_ordered_path: str, exam_file_base_name: list):
    """
    å¯¹å¤šä¸ªè¯•å·ç›®å½•æ‰¹é‡è°ƒç”¨ copy_and_rename_by_similarity_excelï¼Œ
    å‰ææ˜¯æ¯ä¸ªè¯•å·æ–‡ä»¶å¤¹ä¸‹å­˜åœ¨åŒåçš„ "_ç›¸ä¼¼æ€§.xlsx" æ–‡ä»¶ã€‚
    """
    for name in exam_file_base_name:
        file_path = os.path.join(exam_folder_sim_excel_path, name + "_ç›¸ä¼¼æ€§.xlsx")
        print(f"\nğŸ“‚ æ­£åœ¨å¤„ç†ï¼š{name}")

        if os.path.exists(file_path):
            df_file_match = pd.read_excel(file_path)
            print(df_file_match)

            # è°ƒç”¨ä½ å·²æœ‰çš„å‡½æ•°ï¼ˆæ— éœ€é‡å†™ï¼‰
            copy_and_rename_by_similarity_excel(
                input_folder=os.path.join(exam_folder_sim_excel_path, name),
                output_folder=os.path.join(exam_folder_ordered_path, name),
                excel_path=file_path
            )
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")



def rename_txt_files_by_student_info(exam_folder_ordered_path: str, class_folder_name: str):
    """
    å°†æŒ‡å®šç­çº§æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰ .txt æ–‡ä»¶é‡å‘½åä¸ºï¼š
    å­¦å·_å§“å_student_pageN.txt çš„æ ¼å¼
    """
    folder = Path(exam_folder_ordered_path) / class_folder_name

    if not folder.exists():
        print(f"âŒ æ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼š{folder}")
        return

    pattern = re.compile(
        r"^.+?-(\d{10})-([\u4e00-\u9fa5Â·]{1,20})-.*?(student_page\d)\.txt$"
    )
    print(f"âŒ")
    for file in folder.glob("*.txt"):
        match = pattern.match(file.name)
        if match:
            student_id, name, page = match.groups()
            new_name = f"{student_id}_{name}_{page}.txt"
            file.rename(file.with_name(new_name))
            print(f"âœ… é‡å‘½åï¼š{file.name} â†’ {new_name}")
        else:
            print(f"âŒ è·³è¿‡ï¼š{file.name}ï¼ˆæœªåŒ¹é…ï¼‰")



# move best match file according to excel dictionary
def batch_txt_rename_by_student_info(exam_folder_ordered_path: str, exam_file_base_name: list):
    """
    å¯¹å¤šä¸ªè¯•å·ç›®å½•æ‰¹é‡è°ƒç”¨ copy_and_rename_by_similarity_excelï¼Œ
    å‰ææ˜¯æ¯ä¸ªè¯•å·æ–‡ä»¶å¤¹ä¸‹å­˜åœ¨åŒåçš„ "_ç›¸ä¼¼æ€§.xlsx" æ–‡ä»¶ã€‚
    """
    for name in exam_file_base_name:
        # è°ƒç”¨ä½ å·²æœ‰çš„å‡½æ•°ï¼ˆæ— éœ€é‡å†™ï¼‰
        rename_txt_files_by_student_info(
            exam_folder_ordered_path=exam_folder_ordered_path,
            class_folder_name=name
        )