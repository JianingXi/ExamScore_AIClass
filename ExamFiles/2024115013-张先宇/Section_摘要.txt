摘 要
情感分析作为自然语言处理（NLP）的核心任务，在社交媒体监控、客户反馈分析等领域具有重要应用价值。本文针对传统循环神经网络（RNN）在长距离依赖建模效率低、Transformer 模型计算复杂度高的问题，提出一种融合局部上下文编码的轻量化 Transformer 模型（LCT-Transformer）。通过在注意力机制中引入滑动窗口技术，将全局注意力计算复杂度从 O (n²) 降至 O (n・k)（k 为窗口大小，k<<n），同时保留关键语义依赖关系。在公开数据集 IMDb 和自建电商评论数据集上的实验表明，相较于传统 Transformer 模型，LCT-Transformer 的训练速度提升 32%，显存占用减少 45%，在测试集上的准确率达到 92.3%，较 LSTM 模型提高 11.7%，较基础 Transformer 模型提高 3.2%。研究结果为资源受限场景下的情感分析任务提供了高效解决方案。关键词：情感分析；Transformer；轻量化模型；注意力机制；自然语言处理
Title
Optimization of Natural Language Processing for Sentiment Analysis Based on Transformer
NAME
Name-Name 1Department of ****, University, City ZipCode, China
Abstract
Sentiment analysis, a core task in natural language processing (NLP), plays a crucial role in applications such as social media monitoring and customer feedback analysis. Aiming at the inefficiencies of traditional recurrent neural networks (RNNs) in modeling long-distance dependencies and the high computational complexity of the Transformer model, this paper proposes a lightweight Transformer model with local context encoding (LCT-Transformer). By introducing a sliding window technique into the attention mechanism, the computational complexity of global attention is reduced from O(n²) to O(n·k) (where k is the window size, k<<n), while preserving key semantic dependencies. Experiments on the public IMDb dataset and a self-built e-commerce review dataset show that compared with the traditional Transformer model, LCT-Transformer improves training speed by 32%, reduces memory usage by 45%, and achieves a test accuracy of 92.3%, which is 11.7% higher than the LSTM model and 3.2% higher than the basic Transformer model. The results provide an efficient solution for sentiment analysis tasks in resource-constrained scenarios. Key words: sentiment analysis; Transformer; lightweight model; attention mechanism; natural language processing