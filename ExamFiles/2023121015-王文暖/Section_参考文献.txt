参 考 文 献
[1] Brown T B, Mann B, Ryder N, et al. Language Models are Few - Shot Learners [J]. Advances in Neural Information Processing Systems, 2020, 33: 1877 - 1901.
[2] Ramesh A, Pavlov M, Goh G, et al. Zero - Shot Text - to - Image Generation [J]. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, 10261 - 10272.
[3] Saharia C, Chien W, Saxena T, et al. Imagen: Text - to - Image Generation with Text - Conditioned Diffusion Models [J]. 2022.
[4] Jumper J, Evans R, Pritzel A, et al. Highly accurate protein structure prediction with AlphaFold [J]. Nature, 2021, 596 (7873): 583 - 589.
[5] OpenAI. GPT - 4 Technical Report [R]. 2023.
[6] Vaswani A, Shazeer N, Parmar N, et al. Attention Is All You Need[C]//Advances in neural information processing systems. 2017: 5998 - 6008.
[7] Song Y, Meng C, Ermon S. Generative Modeling by Estimating Gradients of the Data Distribution[J]. 2019.
[8] Papineni K, Roukos S, Ward T, et al. BLEU: a Method for Automatic Evaluation of Machine Translation[C]//Proceedings of the 40th annual meeting on association for computational linguistics. 2002: 311 - 318.
[9] Devlin J, Chang M W, Lee K, et al. BERT: Pre - training of Deep Bidirectional Transformers for Language Understanding[C]//Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 2019: 4171 - 4186.
[10] Kingma D P, Ba J. Adam: A Method for Stochastic Optimization[J]. arXiv preprint arXiv:1412.6980, 2014.