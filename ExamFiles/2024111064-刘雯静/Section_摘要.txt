摘  要	针对三维医学影像分割中卷积神经网络感受野受限与计算效率低下的双重挑战，本研究提出TransMed融合架构，通过轴向注意力机制与动态卷积模块的协同设计，在BraTS2023脑肿瘤数据集上实现Dice系数0.891±0.023，较经典U-Net提升9.9%，推理时间降低至1420ms/样本（较传统Transformer模型缩短34.0%）。关键技术贡献包括：① 提出三维轴向注意力机制，通过位置偏置矩阵实现空间编码计算复杂度降低58%；② 设计动态卷积模块，使多尺度特征融合参数量较TransUNet减少44.8%；③ 构建端到端轻量化框架，在NVIDIA A100平台实现显存占用优化42%。实验表明，该模型在HD95距离指标上达到2.31mm，满足临床手术导航精度要求，为AI辅助诊断系统集成提供可行方案。
关键词	Transformer模型；医学影像分割；轴向注意力机制；动态卷积；BraTS2023数据集；模型轻量化；AI辅助诊断
Innovative Research on Transformer-Based Medical Image Segmentation Algorithms
Wenjing Liu
the First Clinical College of Guangzhou Medical University
Abstract	 Addressing the dual challenges of limited receptive fields in convolutional neural networks (CNNs) and low computational efficiency in 3D medical image segmentation, this study proposes the TransMed fusion architecture. Through the synergistic design of an axial attention mechanism and dynamic convolution modules, the model achieves a Dice coefficient of 0.891±0.023 on the BraTS2023 brain tumor dataset, representing a 9.9% improvement over the classical U-Net, while reducing inference time to 1,420 ms per sample (a 34.0% reduction compared to traditional Transformer models). The key technical contributions include: (1) A novel 3D axial attention mechanism that reduces the computational complexity of spatial encoding by 58% through positional bias matrices; (2) A dynamic convolution module that decreases the parameter count for multi-scale feature fusion by 44.8% compared to TransUNet; and (3) An end-to-end lightweight framework optimized for 42% memory footprint reduction on NVIDIA A100 platforms. Experimental results demonstrate that the model achieves an HD95 distance of 2.31 mm, meeting the precision requirements for clinical surgical navigation and providing a feasible solution for integrating AI-assisted diagnostic systems.
The proposed axial attention mechanism decouples the computation of attention across height, width, and depth axes, reducing the computational complexity of traditional global self-attention from \(O((HWD)^2)\) to \(O(HWD(H + W + D))\). For a typical 3D medical image volume of \(128 \times 128 \times 128\), this optimization achieves a 58% reduction in computational load (theoretical 42.7% + 15.3% from mixed-precision training). The dynamic convolution module employs a parameterized kernel generation network to adaptively adjust fusion weights across multi-scale features, compressing parameters from 121.7M in TransUNet to 67.2M while maintaining feature representation capabilities. The lightweight framework integrates gradient checkpointing and automated mixed precision (AMP), reducing GPU memory consumption from 18.6 GB to 10.8 GB on an NVIDIA A100, enabling deployment on standard medical imaging workstations.
Rigorous evaluation on the BraTS2023 dataset (120 multimodal MRI cases, augmented to 600 training samples) demonstrates TransMed’s superiority:
Accuracy: Dice score of 0.891 ± 0.023 (vs. U-Net: 0.81, \(p = 0.017\)), with HD95 of 2.31 mm, outperforming the 3 mm threshold for surgical navigation.
Efficiency: Inference time of 1,420 ms/sample, 34.0% faster than traditional Transformers.
Scalability: Batch size increased to 8 (vs. TransUNet’s maximum of 4 on identical hardware).
Integrated with PACS systems, TransMed improves radiologists’ lesion annotation efficiency by 40.2% (\(n = 15\), annotation time reduced from \(4.7 \pm 1.2\) to \(2.8 \pm 0.6\) minutes/case). Future work will focus on obtaining NMPA Class III certification (1,500 multicenter cases planned) and developing federated learning versions compliant with the EU AI Act’s data governance requirements.
Key words	3D medical image segmentation, axial attention, dynamic convolution, computational efficiency, BraTS2023, surgical navigation.